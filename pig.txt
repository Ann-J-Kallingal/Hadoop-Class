PIG

cd course/softwares

#downloading PIG
wget https://archive.apache.org/dist/pig/pig-0.17.0/pig-0.17.0.tar.gz

#unzipping the pig folder
tar -xzf pig-0.17.0.tar.gz
mv pig-0.17.0 pig

nano ~/.bashrc
export PIG_HOME=$HOME/Desktop/course/softwares/pig
export PATH=$PATH:$PIG_HOME/sbin:$PIG_HOME/bin
export CLASSPATH=$CLASSPATH:$PIG_HOME/lib/*
export PIG_CLASSPATH=$PIG_HOME/conf:$HADOOP_HOME/etc/hadoop
export PIG_CONF_DIR=$PIG_HOME/conf
export PIG_CLASSPATH=$PIG_CONF_DIR:$PATH

source ~/.bashrc

start-dfs.sh
start-yarn.sh
hdfs dfs -put /home/ann007/hadoopdata/empdata.csv /user/data/empdata_pig.csv

pig 


Codes
Copy files to read
hadoop fs -copyFromLocal emp.csv pig/emp.csv 

Load Data

 A = load '/user/root/pig/emp.csv' using PigStorage(',') as (eid:int,ename:chararray,epos:chararray,esal:int,ecom:int,edpno:int);
Dump A;
A2 = load '/user/root/pig/emp.csv’;
describe A2;


Aggregate (by row)

B = filter A by edpno==2;
B2 = filter A by edpno==20 and esal<1000,
C = limit B 3;
D = order C by esal desc;

Store Data
store D into '/pig/pigout1’ using PigStorage(',’);

Transform (by column)

Select existing column
E = foreach A generate eid;

Create new column
F = foreach A generate *,esal*5 as Incentive;

Transform columns
G = foreach A generate SUBSTRING(ename,0,4);


Advanced codes

H = foreach A generate $0,$1;
I = group A by edpno;
J = foreach I generate group as edpno, COUNT($1) as count;
K = foreach A generate MAX(A.esal) as maxsal,MIN(A.esal) as minsal, SUM(A.esal) as sumsal, COUNT($1) as count;
L = group A by (edpno, epos)

SPLIT A into B if edpno==10, C if edpno==20, D if epos=='MANAGER';

Joins

A = load '/emp.csv' using PigStorage(',') as (eid:int,ename:chararray,epos:chararray,esal:int,ecom:int,edpno:int);
B = load '/dept.csv' using PigStorage(',') as (edpno:int,epos:chararray,ecity:chararray);
C = JOIN A by edpno,B by edpno;
D = foreach C generate A::eid,B::epos;
E = JOIN A by edpno RIGHT OUTER, B by edpno;


